
"""
ViralClip Pro v6.0 - Netflix-Level Video Service
Enterprise video processing with advanced performance and scalability
"""

import asyncio
import logging
import time
import uuid
import hashlib
from typing import Dict, List, Any, Optional, Union, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from pathlib import Path
import json
import aiofiles
import weakref
from concurrent.futures import ThreadPoolExecutor
import psutil

from fastapi import UploadFile, HTTPException

logger = logging.getLogger(__name__)


@dataclass
class UploadSession:
    """Enterprise upload session with comprehensive tracking"""
    upload_id: str
    filename: str
    file_size: int
    total_chunks: int
    received_chunks: int = 0
    status: str = "initializing"
    created_at: datetime = field(default_factory=datetime.utcnow)
    last_activity: datetime = field(default_factory=datetime.utcnow)
    user_info: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    performance_metrics: Dict[str, Any] = field(default_factory=dict)


@dataclass
class VideoProcessingResult:
    """Comprehensive video processing result"""
    session_id: str
    success: bool
    processing_time: float
    file_path: Optional[Path] = None
    preview_data: Optional[Dict[str, Any]] = None
    viral_analysis: Optional[Dict[str, Any]] = None
    error_details: Optional[str] = None
    timestamp: datetime = field(default_factory=datetime.utcnow)


class ValidationResult:
    """Upload validation result with detailed feedback"""
    def __init__(self, valid: bool, error: str = None, estimated_time: float = 0):
        self.valid = valid
        self.error = error
        self.estimated_time = estimated_time


class NetflixLevelVideoService:
    """Netflix-level video service with enterprise performance and monitoring"""

    def __init__(self):
        # Performance optimization
        self.thread_pool = ThreadPoolExecutor(max_workers=6, thread_name_prefix="video_service")
        self.upload_sessions: Dict[str, UploadSession] = {}
        self.processing_queue = asyncio.Queue(maxsize=100)
        self.active_processing = weakref.WeakSet()
        
        # Enterprise configuration
        self.max_file_size = 500 * 1024 * 1024  # 500MB
        self.supported_formats = {'.mp4', '.mov', '.avi', '.mkv', '.webm', '.m4v'}
        self.chunk_size = 5 * 1024 * 1024  # 5MB chunks
        self.session_timeout = 3600  # 1 hour
        
        # Performance monitoring
        self.performance_metrics = {
            "total_uploads": 0,
            "successful_uploads": 0,
            "average_upload_time": 0.0,
            "average_processing_time": 0.0,
            "error_rate": 0.0,
            "throughput_mbps": 0.0
        }
        
        # Storage paths
        self.upload_path = Path("nr1copilot/nr1-main/uploads")
        self.temp_path = Path("nr1copilot/nr1-main/temp")
        self.output_path = Path("nr1copilot/nr1-main/output")
        
        # Initialize directories
        for path in [self.upload_path, self.temp_path, self.output_path]:
            path.mkdir(parents=True, exist_ok=True)
        
        logger.info("ðŸŽ¬ Netflix-level video service initialized")

    async def enterprise_warm_up(self):
        """Warm up video service for optimal performance"""
        try:
            start_time = time.time()
            
            # Pre-create processing environment
            await self._setup_processing_environment()
            
            # Initialize codec support
            await self._initialize_codecs()
            
            # Setup monitoring
            await self._setup_performance_monitoring()
            
            warm_up_time = time.time() - start_time
            logger.info(f"ðŸ”¥ Video service warm-up completed in {warm_up_time:.2f}s")
            
        except Exception as e:
            logger.error(f"Video service warm-up failed: {e}", exc_info=True)

    async def validate_enterprise_upload(
        self,
        filename: str,
        file_size: int,
        total_chunks: int,
        user: Dict[str, Any]
    ) -> ValidationResult:
        """Enterprise-level upload validation with comprehensive checks"""
        try:
            # File extension validation
            file_ext = Path(filename).suffix.lower()
            if file_ext not in self.supported_formats:
                return ValidationResult(
                    False, 
                    f"Unsupported format {file_ext}. Supported: {', '.join(self.supported_formats)}"
                )
            
            # File size validation
            if file_size > self.max_file_size:
                return ValidationResult(
                    False,
                    f"File too large ({file_size / 1024 / 1024:.1f}MB). Maximum: {self.max_file_size / 1024 / 1024:.0f}MB"
                )
            
            if file_size <= 0:
                return ValidationResult(False, "Invalid file size")
            
            # Chunk validation
            if total_chunks <= 0 or total_chunks > 1000:
                return ValidationResult(False, "Invalid chunk count")
            
            # User tier validation
            user_tier = user.get("tier", "standard")
            if user_tier == "free" and file_size > 100 * 1024 * 1024:  # 100MB for free
                return ValidationResult(
                    False,
                    "File size exceeds free tier limit (100MB). Please upgrade."
                )
            
            # Rate limiting check
            if not await self._check_user_rate_limit(user):
                return ValidationResult(False, "Rate limit exceeded. Please try again later.")
            
            # Storage capacity check
            if not await self._check_storage_capacity(file_size):
                return ValidationResult(False, "Insufficient storage capacity")
            
            # Estimate processing time
            estimated_time = self._estimate_processing_time(file_size, file_ext)
            
            return ValidationResult(True, None, estimated_time)
            
        except Exception as e:
            logger.error(f"Upload validation failed: {e}", exc_info=True)
            return ValidationResult(False, "Validation failed due to internal error")

    async def create_enterprise_upload_session(
        self,
        upload_id: str,
        filename: str,
        file_size: int,
        total_chunks: int,
        user: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Create enterprise upload session with advanced tracking"""
        try:
            # Create upload session
            session = UploadSession(
                upload_id=upload_id,
                filename=filename,
                file_size=file_size,
                total_chunks=total_chunks,
                user_info=user,
                metadata={
                    "file_extension": Path(filename).suffix.lower(),
                    "estimated_duration": self._estimate_video_duration(file_size),
                    "quality_preset": "auto"
                }
            )
            
            # Store session
            self.upload_sessions[upload_id] = session
            
            # Create upload directory
            upload_dir = self.temp_path / upload_id
            upload_dir.mkdir(exist_ok=True)
            
            logger.info(f"ðŸ“‹ Upload session created: {upload_id} - {filename}")
            
            return {
                "upload_id": upload_id,
                "chunk_size": self.chunk_size,
                "upload_url": f"/api/v6/upload/chunk",
                "session_timeout": self.session_timeout,
                "supported_formats": list(self.supported_formats),
                "status": "ready"
            }
            
        except Exception as e:
            logger.error(f"Failed to create upload session: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail="Session creation failed")

    async def process_enterprise_chunk(
        self,
        file: UploadFile,
        upload_id: str,
        chunk_index: int,
        total_chunks: int,
        chunk_hash: Optional[str],
        user: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Process enterprise chunk with Netflix-level reliability and advanced error handling"""
        try:
            start_time = time.time()
            
            # Validate session
            session = self.upload_sessions.get(upload_id)
            if not session:
                return {"success": False, "error": "Upload session not found", "retry_after": 5}
            
            # Update session activity
            session.last_activity = datetime.utcnow()
            
            # Validate chunk
            if chunk_index < 0 or chunk_index >= total_chunks:
                return {"success": False, "error": "Invalid chunk index"}
            
            # Check for duplicate chunk (resume support)
            chunk_path = self.temp_path / upload_id / f"chunk_{chunk_index:04d}"
            if chunk_path.exists():
                # Chunk already exists, verify integrity
                existing_size = chunk_path.stat().st_size
                session.received_chunks = max(session.received_chunks, chunk_index + 1)
                progress = (session.received_chunks / session.total_chunks) * 100
                
                logger.info(f"ðŸ“¦ Chunk {chunk_index} already exists for {upload_id}")
                
                return {
                    "success": True,
                    "upload_id": upload_id,
                    "chunk_index": chunk_index,
                    "progress": progress,
                    "received_chunks": session.received_chunks,
                    "total_chunks": session.total_chunks,
                    "is_complete": session.received_chunks >= session.total_chunks,
                    "processing_time": 0.001,  # Minimal time for duplicate
                    "duplicate": True,
                    "message": "Chunk already uploaded"
                }
            
            # Read and validate chunk data
            chunk_data = await file.read()
            if not chunk_data:
                return {"success": False, "error": "Empty chunk data", "retry_after": 1}
            
            # Verify chunk size limits
            if len(chunk_data) > self.chunk_size * 2:  # Allow some flexibility
                return {"success": False, "error": f"Chunk too large: {len(chunk_data)} bytes"}
            
            # Verify chunk hash if provided
            if chunk_hash:
                calculated_hash = hashlib.md5(chunk_data).hexdigest()
                if calculated_hash != chunk_hash:
                    return {
                        "success": False, 
                        "error": "Chunk integrity check failed",
                        "expected_hash": chunk_hash,
                        "calculated_hash": calculated_hash,
                        "retry_after": 0.5
                    }
            
            # Ensure upload directory exists
            upload_dir = self.temp_path / upload_id
            upload_dir.mkdir(exist_ok=True)
            
            # Save chunk to disk with atomic write
            temp_chunk_path = chunk_path.with_suffix('.tmp')
            try:
                async with aiofiles.open(temp_chunk_path, 'wb') as f:
                    await f.write(chunk_data)
                
                # Atomic move
                temp_chunk_path.rename(chunk_path)
                
            except Exception as e:
                # Cleanup temp file if it exists
                if temp_chunk_path.exists():
                    temp_chunk_path.unlink()
                raise e
            
            # Update session progress atomically
            old_count = session.received_chunks
            session.received_chunks = max(session.received_chunks, chunk_index + 1)
            progress = (session.received_chunks / session.total_chunks) * 100
            
            # Calculate upload speed
            processing_time = time.time() - start_time
            upload_speed = len(chunk_data) / processing_time if processing_time > 0 else 0
            
            # Update session performance metrics
            if not hasattr(session, 'performance_metrics'):
                session.performance_metrics = {
                    "total_bytes": 0,
                    "total_time": 0,
                    "chunk_times": [],
                    "speed_history": []
                }
            
            session.performance_metrics["total_bytes"] += len(chunk_data)
            session.performance_metrics["total_time"] += processing_time
            session.performance_metrics["chunk_times"].append(processing_time)
            session.performance_metrics["speed_history"].append(upload_speed)
            
            # Keep only last 10 speed measurements
            if len(session.performance_metrics["speed_history"]) > 10:
                session.performance_metrics["speed_history"] = session.performance_metrics["speed_history"][-10:]
            
            # Check if upload is complete
            is_complete = session.received_chunks >= session.total_chunks
            
            result = {
                "success": True,
                "upload_id": upload_id,
                "chunk_index": chunk_index,
                "progress": progress,
                "received_chunks": session.received_chunks,
                "total_chunks": session.total_chunks,
                "is_complete": is_complete,
                "processing_time": processing_time,
                "upload_speed": upload_speed,
                "average_speed": sum(session.performance_metrics["speed_history"]) / len(session.performance_metrics["speed_history"]),
                "estimated_time_remaining": self._calculate_eta(session),
                "chunk_size": len(chunk_data),
                "duplicate": False
            }
            
            # If upload complete, assemble file and start processing
            if is_complete:
                try:
                    await self._finalize_upload(session)
                    result["status"] = "processing_started"
                    result["message"] = "Upload complete! Starting AI analysis..."
                    result["final_file_path"] = str(session.metadata.get("final_path", ""))
                except Exception as e:
                    logger.error(f"Upload finalization failed: {e}")
                    result["success"] = False
                    result["error"] = "Upload finalization failed"
                    result["retry_after"] = 2
            
            # Update global performance metrics
            await self._update_upload_metrics(processing_time, len(chunk_data), True)
            
            # Broadcast progress via WebSocket if available
            if hasattr(self, 'realtime_engine') and self.realtime_engine:
                await self.realtime_engine.broadcast_enterprise_progress(
                    upload_id, result, user
                )
            
            logger.debug(f"âœ… Chunk {chunk_index}/{total_chunks} processed for {upload_id} in {processing_time:.3f}s")
            
            return result
            
        except asyncio.CancelledError:
            # Handle cancellation gracefully
            logger.info(f"Chunk upload cancelled: {upload_id} chunk {chunk_index}")
            return {"success": False, "error": "Upload cancelled", "cancelled": True}
            
        except OSError as e:
            # Handle disk/network errors
            logger.error(f"Storage error during chunk processing: {e}")
            return {
                "success": False, 
                "error": "Storage error", 
                "details": str(e),
                "retry_after": 2
            }
            
        except Exception as e:
            logger.error(f"Chunk processing failed: {e}", exc_info=True)
            await self._update_upload_metrics(0, 0, False)
            return {
                "success": False, 
                "error": "Chunk processing failed",
                "details": str(e),
                "retry_after": 1
            }

    async def generate_preview_with_realtime_feedback(
        self,
        session_id: str,
        start_time: float,
        end_time: float,
        quality: str,
        enable_viral_optimization: bool = True
    ) -> Dict[str, Any]:
        """Generate preview with real-time feedback and viral optimization"""
        try:
            logger.info(f"ðŸŽ¬ Generating preview for session: {session_id}")
            
            # Simulate preview generation stages
            stages = [
                ("initializing", "Initializing preview generation...", 10),
                ("extracting", "Extracting video segment...", 30),
                ("optimizing", "Applying viral optimizations...", 60),
                ("encoding", "Encoding preview...", 80),
                ("finalizing", "Finalizing preview...", 100)
            ]
            
            for stage, message, progress in stages:
                # In production, broadcast to WebSocket connections
                logger.info(f"Preview {session_id}: {message} ({progress}%)")
                await asyncio.sleep(0.5)  # Simulate processing time
            
            # Generate preview metadata
            preview_data = {
                "session_id": session_id,
                "preview_url": f"/api/v6/preview/{session_id}_{start_time}_{end_time}.mp4",
                "thumbnail_url": f"/api/v6/thumbnail/{session_id}_{start_time}.jpg",
                "duration": end_time - start_time,
                "quality": quality,
                "viral_optimizations_applied": enable_viral_optimization,
                "generated_at": datetime.utcnow().isoformat()
            }
            
            if enable_viral_optimization:
                preview_data["optimizations"] = [
                    "Color grading for maximum impact",
                    "Audio enhancement for engagement",
                    "Pacing optimization for retention"
                ]
            
            return preview_data
            
        except Exception as e:
            logger.error(f"Preview generation failed: {e}", exc_info=True)
            raise

    # Private helper methods

    async def _setup_processing_environment(self):
        """Setup optimal processing environment"""
        # Ensure all directories exist
        for path in [self.upload_path, self.temp_path, self.output_path]:
            path.mkdir(parents=True, exist_ok=True, mode=0o755)
        
        # Initialize processing queue worker
        asyncio.create_task(self._process_queue_worker())

    async def _initialize_codecs(self):
        """Initialize video codecs and processing libraries"""
        # Simulate codec initialization
        await asyncio.sleep(0.1)
        logger.info("ðŸŽ¥ Video codecs initialized")

    async def _setup_performance_monitoring(self):
        """Setup performance monitoring"""
        # Start background monitoring task
        asyncio.create_task(self._monitor_performance())

    async def _check_user_rate_limit(self, user: Dict[str, Any]) -> bool:
        """Check user rate limiting"""
        # Implement sophisticated rate limiting
        user_tier = user.get("tier", "standard")
        
        # Different limits for different tiers
        limits = {
            "free": {"uploads_per_hour": 5, "total_mb_per_hour": 500},
            "standard": {"uploads_per_hour": 20, "total_mb_per_hour": 2000},
            "premium": {"uploads_per_hour": 100, "total_mb_per_hour": 10000}
        }
        
        # For demo, always return True
        return True

    async def _check_storage_capacity(self, file_size: int) -> bool:
        """Check available storage capacity"""
        try:
            # Check disk space
            disk_usage = psutil.disk_usage(str(self.upload_path))
            available_space = disk_usage.free
            
            # Require at least 2x file size + 1GB buffer
            required_space = file_size * 2 + (1024 * 1024 * 1024)
            
            return available_space >= required_space
            
        except Exception as e:
            logger.warning(f"Storage capacity check failed: {e}")
            return True  # Fail open

    def _estimate_processing_time(self, file_size: int, file_ext: str) -> float:
        """Estimate processing time based on file characteristics"""
        # Base time per MB
        base_time_per_mb = 2.0  # seconds
        
        # Format multipliers
        format_multipliers = {
            '.mp4': 1.0,
            '.mov': 1.2,
            '.avi': 1.5,
            '.mkv': 1.3,
            '.webm': 1.1,
            '.m4v': 1.0
        }
        
        multiplier = format_multipliers.get(file_ext, 1.5)
        file_size_mb = file_size / (1024 * 1024)
        
        return file_size_mb * base_time_per_mb * multiplier

    def _estimate_video_duration(self, file_size: int) -> float:
        """Estimate video duration from file size"""
        # Rough estimation: 1MB â‰ˆ 1 second for typical video
        return max(1, file_size / (1024 * 1024))

    def _calculate_eta(self, session: UploadSession) -> float:
        """Calculate estimated time remaining for upload"""
        try:
            if not hasattr(session, 'performance_metrics'):
                return 0
            
            metrics = session.performance_metrics
            if not metrics["speed_history"]:
                return 0
            
            # Calculate average speed from recent measurements
            recent_speeds = metrics["speed_history"][-5:]  # Last 5 chunks
            avg_speed = sum(recent_speeds) / len(recent_speeds)
            
            if avg_speed <= 0:
                return 0
            
            # Calculate remaining data
            remaining_chunks = session.total_chunks - session.received_chunks
            estimated_remaining_bytes = remaining_chunks * self.chunk_size
            
            # Estimate time
            eta = estimated_remaining_bytes / avg_speed
            return max(0, eta)
            
        except Exception:
            return 0

    async def _finalize_upload(self, session: UploadSession):
        """Finalize upload by assembling chunks with comprehensive validation"""
        try:
            upload_dir = self.temp_path / session.upload_id
            final_path = self.upload_path / f"{session.upload_id}_{session.filename}"
            
            logger.info(f"ðŸ”„ Starting upload finalization for {session.upload_id}")
            
            # Verify all chunks exist
            missing_chunks = []
            for i in range(session.total_chunks):
                chunk_path = upload_dir / f"chunk_{i:04d}"
                if not chunk_path.exists():
                    missing_chunks.append(i)
            
            if missing_chunks:
                raise Exception(f"Missing chunks: {missing_chunks}")
            
            # Calculate total expected size
            expected_size = session.file_size
            actual_size = 0
            
            # Assemble chunks with progress tracking
            start_time = time.time()
            async with aiofiles.open(final_path, 'wb') as output_file:
                for i in range(session.total_chunks):
                    chunk_path = upload_dir / f"chunk_{i:04d}"
                    
                    # Read and verify chunk
                    async with aiofiles.open(chunk_path, 'rb') as chunk_file:
                        chunk_data = await chunk_file.read()
                        
                        if not chunk_data:
                            raise Exception(f"Empty chunk {i}")
                        
                        actual_size += len(chunk_data)
                        await output_file.write(chunk_data)
                    
                    # Report progress every 10 chunks
                    if i % 10 == 0 or i == session.total_chunks - 1:
                        progress = ((i + 1) / session.total_chunks) * 100
                        logger.debug(f"ðŸ“¦ Assembly progress: {progress:.1f}% ({i+1}/{session.total_chunks})")
            
            assembly_time = time.time() - start_time
            
            # Verify final file size
            if abs(actual_size - expected_size) > 1024:  # Allow 1KB difference
                logger.warning(f"Size mismatch: expected {expected_size}, got {actual_size}")
                # Don't fail, but log the discrepancy
            
            # Verify file integrity if possible
            final_file_size = final_path.stat().st_size
            if final_file_size != actual_size:
                raise Exception(f"File size verification failed: {final_file_size} != {actual_size}")
            
            # Cleanup chunks
            chunks_cleaned = 0
            for chunk_file in upload_dir.glob("chunk_*"):
                try:
                    chunk_file.unlink()
                    chunks_cleaned += 1
                except Exception as e:
                    logger.warning(f"Failed to cleanup chunk {chunk_file}: {e}")
            
            # Remove upload directory
            try:
                upload_dir.rmdir()
            except OSError as e:
                logger.warning(f"Failed to remove upload directory {upload_dir}: {e}")
            
            # Update session with final statistics
            session.status = "completed"
            session.metadata.update({
                "final_path": str(final_path),
                "final_size": final_file_size,
                "assembly_time": assembly_time,
                "chunks_cleaned": chunks_cleaned,
                "total_upload_time": time.time() - session.created_at.timestamp(),
                "average_speed": session.performance_metrics.get("total_bytes", 0) / session.performance_metrics.get("total_time", 1)
            })
            
            # Add to processing queue with priority
            await self.processing_queue.put({
                "type": "video_analysis",
                "session_id": session.upload_id,
                "file_path": final_path,
                "metadata": session.metadata,
                "priority": "high",
                "user_info": session.user_info
            })
            
            # Update performance metrics
            self.performance_metrics["successful_uploads"] += 1
            
            logger.info(
                f"âœ… Upload finalized: {session.upload_id} "
                f"({self.formatFileSize(final_file_size)} in {assembly_time:.2f}s)"
            )
            
        except Exception as e:
            logger.error(f"Upload finalization failed: {e}", exc_info=True)
            session.status = "failed"
            session.metadata["error"] = str(e)
            session.metadata["failed_at"] = datetime.utcnow().isoformat()
            
            # Update error metrics
            self.performance_metrics["failed_uploads"] = self.performance_metrics.get("failed_uploads", 0) + 1
            
            raise e

    def formatFileSize(self, size_bytes: int) -> str:
        """Format file size in human readable format"""
        if size_bytes == 0:
            return "0 B"
        
        size_names = ["B", "KB", "MB", "GB", "TB"]
        import math
        i = int(math.floor(math.log(size_bytes, 1024)))
        p = math.pow(1024, i)
        s = round(size_bytes / p, 2)
        return f"{s} {size_names[i]}"

    async def _process_queue_worker(self):
        """Background worker for processing queue"""
        while True:
            try:
                # Get next item from queue
                item = await self.processing_queue.get()
                
                # Process item
                await self._process_video_item(item)
                
                # Mark task as done
                self.processing_queue.task_done()
                
            except Exception as e:
                logger.error(f"Queue worker error: {e}", exc_info=True)
                await asyncio.sleep(1)

    async def _process_video_item(self, item: Dict[str, Any]):
        """Process individual video item"""
        try:
            session_id = item["session_id"]
            file_path = item["file_path"]
            
            logger.info(f"ðŸŽ¬ Processing video: {session_id}")
            
            # Simulate processing stages
            stages = [
                ("analyzing", "Analyzing video content...", 20),
                ("extracting_features", "Extracting viral features...", 40),
                ("scoring_segments", "Scoring segments...", 60),
                ("generating_timeline", "Generating timeline...", 80),
                ("complete", "Processing complete!", 100)
            ]
            
            for stage, message, progress in stages:
                logger.info(f"Processing {session_id}: {message} ({progress}%)")
                await asyncio.sleep(1)  # Simulate processing time
            
            # Update session
            if session_id in self.upload_sessions:
                self.upload_sessions[session_id].status = "processed"
            
            logger.info(f"âœ… Video processing completed: {session_id}")
            
        except Exception as e:
            logger.error(f"Video processing failed: {e}", exc_info=True)

    async def _monitor_performance(self):
        """Background performance monitoring"""
        while True:
            try:
                # Monitor system resources
                memory_info = psutil.Process().memory_info()
                cpu_percent = psutil.cpu_percent(interval=1)
                
                # Log performance metrics
                logger.info(
                    f"Performance: CPU {cpu_percent}%, "
                    f"Memory {memory_info.rss / 1024 / 1024:.1f}MB, "
                    f"Active sessions: {len(self.upload_sessions)}"
                )
                
                # Cleanup expired sessions
                await self._cleanup_expired_sessions()
                
                await asyncio.sleep(60)  # Monitor every minute
                
            except Exception as e:
                logger.error(f"Performance monitoring error: {e}")
                await asyncio.sleep(60)

    async def _cleanup_expired_sessions(self):
        """Clean up expired upload sessions"""
        current_time = datetime.utcnow()
        expired_sessions = []
        
        for upload_id, session in self.upload_sessions.items():
            if current_time - session.last_activity > timedelta(seconds=self.session_timeout):
                expired_sessions.append(upload_id)
        
        for upload_id in expired_sessions:
            # Cleanup files
            temp_dir = self.temp_path / upload_id
            if temp_dir.exists():
                for file in temp_dir.glob("*"):
                    file.unlink()
                temp_dir.rmdir()
            
            # Remove session
            del self.upload_sessions[upload_id]
        
        if expired_sessions:
            logger.info(f"ðŸ§¹ Cleaned up {len(expired_sessions)} expired sessions")

    async def _update_upload_metrics(self, processing_time: float, data_size: int, success: bool):
        """Update upload performance metrics"""
        self.performance_metrics["total_uploads"] += 1
        
        if success:
            self.performance_metrics["successful_uploads"] += 1
            
            # Update average upload time
            current_avg = self.performance_metrics["average_upload_time"]
            total = self.performance_metrics["total_uploads"]
            self.performance_metrics["average_upload_time"] = (
                (current_avg * (total - 1) + processing_time) / total
            )
            
            # Update throughput
            if processing_time > 0:
                mbps = (data_size / 1024 / 1024) / processing_time
                current_throughput = self.performance_metrics["throughput_mbps"]
                self.performance_metrics["throughput_mbps"] = (
                    (current_throughput * (total - 1) + mbps) / total
                )
        
        # Update error rate
        success_rate = self.performance_metrics["successful_uploads"] / self.performance_metrics["total_uploads"]
        self.performance_metrics["error_rate"] = 1.0 - success_rate

    async def get_performance_metrics(self) -> Dict[str, Any]:
        """Get current performance metrics"""
        return {
            **self.performance_metrics,
            "active_sessions": len(self.upload_sessions),
            "queue_size": self.processing_queue.qsize(),
            "memory_usage_mb": psutil.Process().memory_info().rss / 1024 / 1024,
            "cpu_percent": psutil.cpu_percent(),
            "timestamp": datetime.utcnow().isoformat()
        }

    async def get_session_status(self, upload_id: str) -> Optional[Dict[str, Any]]:
        """Get upload session status"""
        session = self.upload_sessions.get(upload_id)
        if not session:
            return None
        
        return {
            "upload_id": session.upload_id,
            "filename": session.filename,
            "status": session.status,
            "progress": (session.received_chunks / session.total_chunks) * 100,
            "received_chunks": session.received_chunks,
            "total_chunks": session.total_chunks,
            "created_at": session.created_at.isoformat(),
            "last_activity": session.last_activity.isoformat(),
            "metadata": session.metadata
        }

    async def graceful_shutdown(self):
        """Gracefully shutdown the video service"""
        logger.info("ðŸ”„ Shutting down video service...")
        
        # Stop processing queue
        while not self.processing_queue.empty():
            try:
                self.processing_queue.get_nowait()
                self.processing_queue.task_done()
            except:
                break
        
        # Shutdown thread pool
        self.thread_pool.shutdown(wait=True)
        
        # Cleanup temporary files
        for session in self.upload_sessions.values():
            temp_dir = self.temp_path / session.upload_id
            if temp_dir.exists():
                for file in temp_dir.glob("*"):
                    file.unlink()
                temp_dir.rmdir()
        
        # Clear sessions
        self.upload_sessions.clear()
        
        logger.info("âœ… Video service shutdown complete")
